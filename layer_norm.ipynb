{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070de98f",
   "metadata": {},
   "source": [
    "LayerNorm is calculated for vectors, broadcasted on batches. For each vector, it is\n",
    "\n",
    "$y_i = w_i * \\hat x_i + b_i$, where $\\hat x_i$ is the normalized input.\n",
    "\n",
    "For back prop, recall\n",
    "\n",
    "$$\n",
    "\\hat{x}_a = \\frac{x_a - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{N} \\sum_{j=1}^N x_j, \\quad \\sigma = \\sqrt{\\frac{1}{N} \\sum_{j=1}^N (x_j - \\mu)^2 + \\epsilon}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\sigma}{\\partial x_i} = \\dfrac{1}{N\\sigma}\\sum_j (x_j-\\mu)\\left(\\delta_{ij}-\\dfrac{1}{N}\\right) = \\dfrac{x_i-\\mu}{N\\sigma}\n",
    "$$\n",
    "\n",
    "therefore, for each input vector,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial x_i} &= \\sum_{a, b}\\dfrac{\\partial L}{\\partial y_a}\\dfrac{\\partial y_a}{\\partial\\hat x_b}\\dfrac{\\partial\\hat x_b}{\\partial x_i}\\\\\n",
    "&= \\sum_{a}\\dfrac{\\partial L}{\\partial y_a}w_a\\dfrac{\\partial\\hat x_a}{\\partial x_i}\\\\\n",
    "&= \\sum_{a}\\dfrac{\\partial L}{\\partial y_a}w_a\\left(\\dfrac{1}{\\sigma}\\left(\\delta_{a,i}-\\dfrac{1}{N}-\\dfrac{\\hat x_a\\hat x_i}{N}\\right)\\right)\\\\\n",
    "&=\\dfrac{1}{\\sigma}\\left(\\dfrac{\\partial L}{\\partial y_i}w_i-\\dfrac{1}{N}\\sum_aw_a\\dfrac{\\partial L}{\\partial y_a}-\\dfrac{1}{N}\\sum_aw_a\\dfrac{\\partial L}{\\partial y_a}\\hat x_a\\hat x_i\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Gradients for learnable weights and biases:\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w_i} = \\sum_a\\dfrac{\\partial L}{\\partial y_a}\\dfrac{\\partial y_a}{\\partial w_i} = \\sum_a\\dfrac{\\partial L}{\\partial y_a}x_i$\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial b_i} = \\sum_a\\dfrac{\\partial L}{\\partial y_a}\\dfrac{\\partial y_a}{\\partial b_i} = \\dfrac{\\partial L}{\\partial y_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6408f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    " \n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7881675",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def layer_norm(\n",
    "    X,  # input\n",
    "    Y,  # output\n",
    "    W,  # weights\n",
    "    B,  # biases\n",
    "    Mean,  # means\n",
    "    Rstd,  # 1/stds\n",
    "    M,  # number of rows in X\n",
    "    N,  # number of columns in X\n",
    "    eps,  # epsilon to avoid division by zero\n",
    "    BLOCK_SIZE: tl.constexpr, # divide columns into blocks\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    X += pid * N\n",
    "    Y += pid * N\n",
    "    # compute mean\n",
    "    mean = 0\n",
    "    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
    "    for off in range(0, N, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n",
    "        _mean += a\n",
    "    mean = tl.sum(_mean, axis=0) / N\n",
    "    # compute variance\n",
    "    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
    "    for off in range(0, N, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n",
    "        x = tl.where(cols < N, x - mean, 0.)\n",
    "        _var += x * x\n",
    "    var = tl.sum(_var, axis=0) / N\n",
    "    rstd = 1 / tl.sqrt(var + eps)\n",
    "    # write mean / rstd\n",
    "    tl.store(Mean + pid, mean)\n",
    "    tl.store(Rstd + pid, rstd)\n",
    "    # layer normalization\n",
    "    for off in range(0, N, BLOCK_SIZE):\n",
    "        cols = off + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = cols < N\n",
    "        w = tl.load(W + cols, mask=mask)\n",
    "        b = tl.load(B + cols, mask=mask)\n",
    "        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n",
    "        x_hat = (x - mean) * rstd\n",
    "        # element-wise product & addition\n",
    "        y = x_hat * w + b\n",
    "        # Write output\n",
    "        tl.store(Y + cols, y, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e1f58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
