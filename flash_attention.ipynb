{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe267337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# not on blackwell / hopper, using raw ptrs (not descriptors), no FP8\n",
    "# my triton is old - descriptors are still experimental!\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd (\n",
    "   scale, # scaling factor 1/sqrt(d_k)\n",
    "   ptr_b, # output tensor to store \\log\\sum_j\\exp(A_{ij}) per row \n",
    "         # to be used in backward \n",
    "         # dimension: (Z * H * N, 1)\n",
    "   Z, # batch size\n",
    "   H, # number of heads\n",
    "   N, # number of tokens\n",
    "   ptr_q, # pointer to Q (Z * H * N, HEAD_DIM)\n",
    "         # each row of Q corresponds to a query from a specific token in a specific head & batch\n",
    "   ptr_k, # pointer to K\n",
    "   ptr_v, # pointer to V (d_v = d_k)\n",
    "   ptr_o, # pointer to O\n",
    "   HEAD_DIM: tl.constexpr, # d^h_k\n",
    "   BLOCK_M: tl.constexpr, # tile size in query direction\n",
    "   BLOCK_N: tl.constexpr, # ... in token sequence direction\n",
    "   STAGE: tl.constexpr, # flash stage\n",
    "):\n",
    "   pid_m = tl.program_id(0) # row-tile block-id (which BLOCK_M of the Query for a specific batch & head)\n",
    "   pid_hz = tl.program_id(1) # which batch and head we are in\n",
    "   # we could use a 3D launch grid, but 2D might have slightly less overhead for triton\n",
    "   # batch & head id\n",
    "   pid_z = pid_hz // H\n",
    "   pid_h = pid_hz % H\n",
    "\n",
    "   # the range of the current head\n",
    "   head_start = pid_z * (H * N) + pid_h * N\n",
    "   head_end = head_start + N\n",
    "\n",
    "   # row offset: from head_start, moving down pid_m blocks\n",
    "   off_m = pid_m * BLOCK_M + head_start\n",
    "   offs_m = tl.arange(0, BLOCK_M)\n",
    "\n",
    "   # initialize running statistics (sftmax) in SRAM / Registers\n",
    "   max_r = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "   expsum_r = tl.zeros([BLOCK_M], dtype=tl.float32) + 0.0\n",
    "   output = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "   # becasue we use powers of 2 (faster) and exp(e) = 2^(x * log_2e)\n",
    "   scale = scale * 1.44269504\n",
    "\n",
    "   offs_row = off_m + offs_m  # shape (BLOCK_M,)\n",
    "   offs_col = tl.arange(0, HEAD_DIM) # shape (HEAD_DIM,)\n",
    "   \n",
    "   q = tl.load(ptr_q + offs_row[:, None] * HEAD_DIM + offs_col[None, :],\n",
    "               mask=(offs_row < head_end)[:, None], other=0.0)\n",
    "   \n",
    "   # stage 1 (off-band)\n",
    "   if STAGE == 1:\n",
    "      low, high = 0, pid_m * BLOCK_M\n",
    "   # stage 2 (on-band)\n",
    "   elif STAGE == 2:\n",
    "      low, high = pid_m * BLOCK_M, min((pid_m + 1) * BLOCK_M, N)\n",
    "   # stage 3 (disable mask)\n",
    "   else:\n",
    "      low, high = 0, N\n",
    "\n",
    "   offs_n = tl.arange(0, BLOCK_N)\n",
    "\n",
    "   # iterate over K, V   \n",
    "   for off_n in tl.range(low, high, BLOCK_N):\n",
    "      offs_row_kv = head_start + off_n + offs_n\n",
    "      offs_row_kv_mask = off_n + offs_n < high\n",
    "      k = tl.load(ptr_k + offs_row_kv[:, None] * HEAD_DIM + offs_col,\n",
    "                  mask=(offs_row_kv_mask[:,None]), other=0.0)\n",
    "      v = tl.load(ptr_v + offs_row_kv[:, None] * HEAD_DIM + offs_col,\n",
    "                  mask=(offs_row_kv_mask[:,None]), other=0.0)\n",
    "      k = k.T\n",
    "      # dot product of Q [pid_m] x K [low - high]\n",
    "      qk = tl.dot(q, k)\n",
    "      if STAGE == 2:\n",
    "         causal_mask = offs_m[:, None] >= (off_n + offs_n[None, :])\n",
    "         qk = qk * scale + tl.where(causal_mask, 0, -1e-6)\n",
    "      else:\n",
    "         qk = qk * scale\n",
    "      # maximum is at least 1 (for numeric stability)\n",
    "      max_r_now = tl.maximum(max_r, tl.max(qk, 1))\n",
    "      qk = qk - max_r_now[:, None]\n",
    "      exp_qk = tl.math.exp2(qk)\n",
    "      alpha = tl.math.exp2(max_r - max_r_now)\n",
    "      # sum across columns - row-wise sum\n",
    "      expsum_r_now = tl.sum(exp_qk, 1)\n",
    "      \n",
    "      # scale output for the new max\n",
    "      output = output * alpha\n",
    "\n",
    "      exp_qk = exp_qk.to(tl.float16)\n",
    "      # for evert entry, compute and add weight-avged values\n",
    "      # use tensor cores: exp_qk(f16), v(f16), output(f32)\n",
    "      output = tl.dot(exp_qk, v, output)\n",
    "\n",
    "      expsum_r = expsum_r * alpha + expsum_r_now\n",
    "      max_r = max_r_now\n",
    "\n",
    "      # sometimes putting things at the end of the loop\n",
    "      # so that variable lifetimes overlap less\n",
    "      # might reduce register pressure\n",
    "      # not the case here because most variables are already alive\n",
    "   \n",
    "   output = output / expsum_r[:, None]\n",
    "   tl.store(ptr_b + offs_row, \n",
    "            max_r + tl.math.log2(expsum_r),\n",
    "            mask=(offs_row < head_end))\n",
    "   tl.store(ptr_o + offs_row[:, None] * HEAD_DIM + offs_col[None, :], \n",
    "            output.to(tl.float16),\n",
    "            mask=(offs_row < head_end)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbb4a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abs diff: 2.407470703125\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 476 / 512 (93.0%)\nGreatest absolute difference: 2.407470703125 at index (0, 0, 0, 0) (up to 0.01 allowed)\nGreatest relative difference: 446.02557373046875 at index (0, 0, 24, 6) (up to 0.01 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Triton forward pass matches PyTorch SDPA (causal).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mtest_attn_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtest_attn_fwd\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMax abs diff:\u001b[39m\u001b[33m\"\u001b[39m, (O - ref).abs().max().item())\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# assertion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Triton forward pass matches PyTorch SDPA (causal).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new/lib/python3.12/site-packages/torch/testing/_comparison.py:1519\u001b[39m, in \u001b[36massert_close\u001b[39m\u001b[34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[39m\n\u001b[32m   1497\u001b[39m error_metas = not_close_error_metas(\n\u001b[32m   1498\u001b[39m     actual,\n\u001b[32m   1499\u001b[39m     expected,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m     msg=msg,\n\u001b[32m   1515\u001b[39m )\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[32m   1518\u001b[39m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[32m0\u001b[39m].to_error(msg)\n",
      "\u001b[31mAssertionError\u001b[39m: Tensor-likes are not close!\n\nMismatched elements: 476 / 512 (93.0%)\nGreatest absolute difference: 2.407470703125 at index (0, 0, 0, 0) (up to 0.01 allowed)\nGreatest relative difference: 446.02557373046875 at index (0, 0, 24, 6) (up to 0.01 allowed)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "\n",
    "\n",
    "def test_attn_fwd():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # dimensions\n",
    "    Z, H, N, D = 1, 1, 32, 16\n",
    "    scale = 1.0 / math.sqrt(D)\n",
    "    BLOCK_M, BLOCK_N = 16, 16\n",
    "\n",
    "    # inputs\n",
    "    Q = torch.randn(Z, H, N, D, dtype=torch.float16, device=\"cuda\").contiguous()\n",
    "    K = torch.randn(Z, H, N, D, dtype=torch.float16, device=\"cuda\").contiguous()\n",
    "    V = torch.randn(Z, H, N, D, dtype=torch.float16, device=\"cuda\").contiguous()\n",
    "\n",
    "    Q_flat = Q.view(-1, D)\n",
    "    K_flat = K.view(-1, D)\n",
    "    V_flat = V.view(-1, D)\n",
    "\n",
    "    O = torch.empty_like(Q_flat)\n",
    "    B = torch.empty(Z * H * N, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # stage 1 + 2\n",
    "    _attn_fwd[(N // BLOCK_M, Z * H)](\n",
    "        scale, B, Z, H, N,\n",
    "        Q_flat, K_flat, V_flat, O,\n",
    "        D, BLOCK_M, BLOCK_N, 1\n",
    "    )\n",
    "\n",
    "    _attn_fwd[(N // BLOCK_M, Z * H)](\n",
    "        scale, B, Z, H, N,\n",
    "        Q_flat, K_flat, V_flat, O,\n",
    "        D, BLOCK_M, BLOCK_N, 2\n",
    "    )\n",
    "\n",
    "    # reshape for comparison\n",
    "    O = O.view(Z, H, N, D).float()\n",
    "\n",
    "    # reference using PyTorch’s built-in causal attention\n",
    "    ref = torch.nn.functional.scaled_dot_product_attention(\n",
    "        Q.float(), K.float(), V.float(), attn_mask=None, dropout_p=0.0, is_causal=True\n",
    "    )\n",
    "\n",
    "    # debugging output\n",
    "    print(\"Max abs diff:\", (O - ref).abs().max().item())\n",
    "\n",
    "    # assertion\n",
    "    torch.testing.assert_close(O, ref, atol=1e-2, rtol=1e-2)\n",
    "    print(\"✅ Triton forward pass matches PyTorch SDPA (causal).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_attn_fwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1b9df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16]) torch.Size([32, 16])\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 3.1650e-05, -1.5646e-06,  3.1269e-04, -1.5706e-04,  1.2660e-04,\n",
      "         -1.7602e-06,  1.3423e-04,  3.4559e-04, -4.8637e-04,  3.5822e-04,\n",
      "          2.3979e-04, -2.3025e-04, -1.7858e-04, -1.6493e-04, -4.9144e-05,\n",
      "          6.2346e-05],\n",
      "        [ 1.4186e-04, -6.9678e-05,  3.2961e-05,  1.4424e-05, -7.7724e-05,\n",
      "         -1.2815e-04, -2.9504e-05, -9.5367e-05, -6.2481e-05,  7.3850e-05,\n",
      "          2.5499e-04,  2.9884e-05,  4.0257e-04, -8.8066e-05, -8.8274e-05,\n",
      "         -1.1822e-04],\n",
      "        [-2.4164e-04,  1.1872e-04,  2.2233e-05,  3.3176e-04,  6.6668e-05,\n",
      "         -1.4722e-05, -2.9621e-04,  1.3143e-04,  9.2298e-05, -1.0931e-04,\n",
      "         -4.5502e-04, -5.5596e-05,  2.1142e-04, -3.2619e-05, -1.0592e-04,\n",
      "          3.4639e-04],\n",
      "        [-1.4937e-04, -3.0726e-05,  1.0222e-04, -4.1431e-04, -2.2650e-06,\n",
      "          1.4886e-04, -8.5980e-06,  2.0385e-05,  1.4573e-04, -1.1823e-04,\n",
      "          9.7752e-06, -1.4234e-04, -4.8912e-04,  1.1623e-05, -1.7366e-04,\n",
      "          1.0443e-04],\n",
      "        [-3.3528e-05,  3.0259e-06,  2.4527e-04, -2.0105e-04, -2.2799e-05,\n",
      "         -5.4240e-05, -8.4639e-05,  4.6790e-06,  3.2604e-04,  2.4748e-04,\n",
      "          2.1529e-04, -1.4907e-04,  2.1535e-04,  8.7917e-05, -7.4953e-05,\n",
      "          1.1790e-04],\n",
      "        [ 1.0693e-04,  4.0531e-06, -4.7088e-04,  6.9752e-05, -4.0509e-05,\n",
      "         -1.8290e-05,  4.6372e-05,  1.0184e-05,  4.7207e-05,  1.2513e-04,\n",
      "          1.5214e-05, -3.8743e-05,  3.9548e-04,  6.9469e-05,  4.4659e-05,\n",
      "         -4.6268e-05],\n",
      "        [ 6.1244e-05, -7.2062e-05, -1.1253e-04, -2.6906e-04, -4.2021e-05,\n",
      "         -2.1718e-05, -6.5424e-05,  4.1276e-06,  3.6098e-06,  1.9968e-06,\n",
      "          1.1122e-04, -6.2585e-06,  2.6762e-04, -9.7767e-05, -1.1629e-04,\n",
      "          5.6103e-05],\n",
      "        [ 1.4603e-06, -5.4121e-05,  4.4391e-05,  1.2344e-04, -7.9056e-05,\n",
      "          6.4403e-05,  9.4801e-05, -4.0233e-06,  7.3075e-05, -1.7881e-05,\n",
      "          4.1902e-05,  1.0133e-05,  4.7922e-05, -2.4945e-05, -1.7717e-05,\n",
      "         -4.9353e-05],\n",
      "        [ 1.5396e-04, -2.0739e-04,  6.2883e-05,  1.5110e-04, -1.7455e-04,\n",
      "          7.7486e-06, -5.6177e-05,  4.5069e-05,  8.0705e-05,  8.8513e-06,\n",
      "         -2.0629e-04,  1.7774e-04,  5.6982e-05, -8.8558e-05, -1.2136e-04,\n",
      "          1.1998e-04],\n",
      "        [ 1.0529e-04,  1.7703e-05,  2.7382e-04,  6.4671e-06,  1.3530e-04,\n",
      "          2.8610e-04,  8.7708e-05,  1.1118e-04,  8.1837e-05, -7.2904e-05,\n",
      "          1.0580e-04,  1.8120e-05,  5.9009e-06, -5.7764e-05, -3.5971e-05,\n",
      "          7.3135e-05],\n",
      "        [ 2.8580e-05,  8.6457e-05,  8.6069e-05, -1.2144e-05,  1.3254e-04,\n",
      "         -5.2989e-05, -4.3482e-05,  5.0575e-05, -8.2932e-05,  2.1473e-05,\n",
      "         -1.4305e-04,  2.9266e-05,  2.6721e-04,  1.0282e-04,  1.2138e-04,\n",
      "          1.5482e-05],\n",
      "        [-1.2541e-04,  2.4706e-05,  8.8859e-04,  8.7902e-05, -1.7285e-05,\n",
      "         -2.5712e-05, -2.5809e-05, -2.9802e-08,  9.1791e-06,  7.9520e-05,\n",
      "         -1.3897e-04,  1.2505e-04, -2.2352e-04,  7.1213e-05, -6.3270e-05,\n",
      "          2.6435e-04],\n",
      "        [-3.0205e-05, -1.1611e-04, -1.0736e-05, -3.2842e-05, -1.1186e-04,\n",
      "          2.2316e-04, -2.1249e-05,  1.3558e-04,  4.6745e-05, -2.2747e-04,\n",
      "         -1.1170e-04, -9.9719e-05,  2.1040e-05, -3.0011e-05, -5.2363e-05,\n",
      "          4.5836e-05],\n",
      "        [ 1.1094e-05,  3.1501e-05, -4.8566e-04,  1.8728e-04,  1.0100e-04,\n",
      "         -4.4346e-05, -1.5423e-05, -1.8686e-04, -5.4836e-06,  1.5959e-05,\n",
      "          4.8041e-05, -2.3636e-05,  1.6695e-04, -2.2769e-05,  9.5740e-06,\n",
      "          1.3830e-05],\n",
      "        [ 3.1590e-05, -1.3977e-04, -8.7440e-05, -1.3024e-04, -1.1289e-04,\n",
      "         -1.3918e-05, -7.8440e-05, -4.0285e-05, -8.7380e-05,  1.8942e-04,\n",
      "          4.6074e-04, -9.6217e-05, -2.5284e-04, -6.1527e-05, -3.1061e-05,\n",
      "          1.3071e-04],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan,\n",
      "                 nan]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The values for attribute 'dtype' do not match: torch.float16 != torch.float32.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Triton forward pass matches PyTorch SDPA (causal).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43mtest_attn_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtest_attn_fwd\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(ref.shape, O.shape)\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(O-ref)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Triton forward pass matches PyTorch SDPA (causal).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new/lib/python3.12/site-packages/torch/testing/_comparison.py:1519\u001b[39m, in \u001b[36massert_close\u001b[39m\u001b[34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[39m\n\u001b[32m   1497\u001b[39m error_metas = not_close_error_metas(\n\u001b[32m   1498\u001b[39m     actual,\n\u001b[32m   1499\u001b[39m     expected,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m     msg=msg,\n\u001b[32m   1515\u001b[39m )\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[32m   1518\u001b[39m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[32m0\u001b[39m].to_error(msg)\n",
      "\u001b[31mAssertionError\u001b[39m: The values for attribute 'dtype' do not match: torch.float16 != torch.float32."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_attn_fwd():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # dimensions\n",
    "    Z, H, N, D = 1, 1, 32, 16\n",
    "    scale = 1.0 / math.sqrt(D)\n",
    "    BLOCK_M, BLOCK_N = 16, 16\n",
    "\n",
    "    # inputs\n",
    "    Q = torch.randn(Z, H, N, D, dtype=torch.float16, device=\"cuda\").contiguous()\n",
    "    K = torch.randn(Z, H, N, D, dtype=torch.float16, device=\"cuda\").contiguous()\n",
    "    V = torch.randn(Z, H, N, D, dtype=torch.float16, device=\"cuda\").contiguous()\n",
    "\n",
    "    Q_flat = Q.view(-1, D)\n",
    "    K_flat = K.view(-1, D)\n",
    "    V_flat = V.view(-1, D)\n",
    "\n",
    "    O = torch.empty_like(Q_flat)\n",
    "    B = torch.empty(Z * H * N, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # stage 1 + 2\n",
    "    _attn_fwd[(N // BLOCK_M, Z * H)](\n",
    "        scale, B, Z, H, N,\n",
    "        Q_flat, K_flat, V_flat, O,\n",
    "        D, BLOCK_M, BLOCK_N, 1\n",
    "    )\n",
    "\n",
    "    _attn_fwd[(N // BLOCK_M, Z * H)](\n",
    "        scale, B, Z, H, N,\n",
    "        Q_flat, K_flat, V_flat, O,\n",
    "        D, BLOCK_M, BLOCK_N, 2\n",
    "    )\n",
    "\n",
    "    # reference using PyTorch’s built-in causal attention\n",
    "    ref = torch.nn.functional.scaled_dot_product_attention(\n",
    "        Q.float(), K.float(), V.float(), attn_mask=None, dropout_p=0.0, is_causal=True\n",
    "    )\n",
    "\n",
    "\n",
    "    ref = ref.view(-1, D)\n",
    "\n",
    "    print(ref.shape, O.shape)\n",
    "\n",
    "    print(O-ref)\n",
    "    torch.testing.assert_close(O, ref, atol=1e-2, rtol=1e-2)\n",
    "    print(\"✅ Triton forward pass matches PyTorch SDPA (causal).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_attn_fwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
