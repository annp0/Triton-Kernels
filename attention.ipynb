{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa7bd6f",
   "metadata": {},
   "source": [
    "### -1. Back prop for matmul\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "Y = WX, \\quad y_{ij} = \\sum_a w_{ia}x_{aj}\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial y_{ij}}{\\partial x_{mn}}&=\\dfrac{\\partial}{\\partial x_{mn}}\\left(\\sum_aw_{ia}x_{aj}\\right)\\\\\n",
    "&=\\sum_a\\dfrac{\\partial}{\\partial x_{mn}}w_{ia}x_{aj}\\\\\n",
    "&=\\sum_a w_{ia}\\delta_{m,a}\\delta_{n,j}\\\\\n",
    "&=w_{im}\\delta_{n,j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial x_{mn}}&=\\sum_{ij}\\dfrac{\\partial L}{\\partial y_{ij}}\\dfrac{\\partial y_{ij}}{\\partial x_{mn}}\\\\\n",
    "&=\\sum_{ij}\\dfrac{\\partial L}{\\partial y_{ij}}w_{im}\\delta_{n,j}\\\\\n",
    "&=\\sum_{i}\\dfrac{\\partial L}{\\partial y_{in}}w_{im}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\left(\\dfrac{\\partial L}{\\partial X}\\right)=W^T\\left(\\dfrac{\\partial L}{\\partial Y}\\right)\n",
    "$$\n",
    "\n",
    "and similarly,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W}=\\dfrac{\\partial L}{\\partial Y} X^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd752e1",
   "metadata": {},
   "source": [
    "#### 0. Input and embedding\n",
    "\n",
    "Let:\n",
    "\n",
    "* $I \\in \\mathbb{Z}^{T}$: token indices of the input sequence\n",
    "* Embedding matrix: $E_{\\text{lookup}} \\in \\mathbb{R}^{d_{\\text{token}} \\times d_{\\text{model}}}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "X = E_{\\text{lookup}}[I] \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "This means that each token index in $I$ is used to select a row from $E_{\\text{lookup}}$.\n",
    "\n",
    "Note that this is not a matmul (it can be written as a matmul if $I$ was one-hot encoded), it is just a lookup. However, autograd engines can track which row it used and back prop gradients to that row.\n",
    "\n",
    "#### 1. Linear Projections\n",
    "\n",
    "We define three learnable projection matrices:\n",
    "\n",
    "* $W_Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "* $W_K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "* $W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n",
    "\n",
    "Then the projections are:\n",
    "\n",
    "$$\n",
    "Q = X W_Q \\in \\mathbb{R}^{T \\times d_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K = X W_K \\in \\mathbb{R}^{T \\times d_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V = X W_V \\in \\mathbb{R}^{T \\times d_v}\n",
    "$$\n",
    "\n",
    "The dimension $k$ is usually taken to be even for RoPE and acceleration. Each token corresponds to a row.\n",
    "\n",
    "#### 2. RoPE\n",
    "\n",
    "Positional Embeddings are necessary to make sure the model is position-ware to inputs. For each token position $t\\in\\{1,\\cdots,T\\}$, and each dimension pair $i\\in\\{1,\\cdots, d_k/2\\}$ (arrays start from 1), define\n",
    "\n",
    "$$\n",
    "q_{t,i} = \\left[\\begin{matrix}\n",
    "Q_{t, 2i-1}\\\\ Q_{t,2i}\n",
    "\\end{matrix}\\right],\\quad\n",
    "k_{t,i} = \\left[\\begin{matrix}\n",
    "K_{t, 2i-1}\\\\ K_{t,2i}\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "and define\n",
    "\n",
    "$$\n",
    "R(i, t) = \\left[\\begin{matrix}\n",
    "\\cos(\\omega_i t) & -\\sin(\\omega_i t)\\\\\n",
    "\\sin(\\omega_i t) & \\cos(\\omega_i t)\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "q'_{t,i}=R(i, t)q_{t,i},\\quad k'_{t,i}=R(i, t)k_{t,i}\n",
    "$$\n",
    "\n",
    "and we put it back into $Q', K'$.\n",
    "\n",
    "We use RoPE mainly because of the property where the dot product only relies on the original dot product and the relative position.\n",
    "\n",
    "#### 3. Naive Attention\n",
    "\n",
    "We compute attention weights:\n",
    "\n",
    "$$\n",
    "A = \\frac{Q' K'^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{T \\times T}\n",
    "$$\n",
    "\n",
    "and $A_{ij}$ is basically the dot product of the query of token $i$ and the key of token $j$. So now the $i$-th row of $A$ is the 'score' of keys of other tokens against token $i$. We apply row-wise softmax to convert them to weights that adds to 1:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp\\left( A_{ij} \\right)}{\\sum_{k=1}^{T} \\exp\\left( A_{ik} \\right)}\n",
    "$$\n",
    "\n",
    "then we use this to average against $V$:\n",
    "\n",
    "$$\n",
    "O = \\alpha V \\in \\mathbb{R}^{T\\times d_v}\n",
    "$$\n",
    "\n",
    "#### 4. Masks\n",
    "\n",
    "Before applying softmax, we may need to mask out certain entries in $A$ to force better behaviour.\n",
    "\n",
    "(a) Causal Mask: For auto-regressive generation, the output of token $i$ should only use the values of tokens $j\\le i$, since the future is not avaliable at inference-time. Define \n",
    "\n",
    "$$\n",
    "(M_c)_{ij}=\\begin{cases}\n",
    "0, & j\\le i\\\\\n",
    "-\\inf, &j > i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and let $A'=A+M_c$, this effective turns the weights / scores of masked entries to zero.\n",
    "\n",
    "(b) Padding Mask: If padding tokens were to be used, they would be meaningless, therefore, we zero out every entry whose indices contains a padding token.\n",
    "\n",
    "For the autograd engine, the gradient will only flow to non-masked entries:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial O} V^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A'_{ij}}=\\sum_{ab}\\dfrac{\\partial L}{\\partial \\alpha_{ab}}\\dfrac{\\partial \\alpha_{ab}}{\\partial A'_{ij}}\n",
    "$$\n",
    "\n",
    "Since the softmax was applied row-wise, within the same row we have\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\alpha_{ij}}{\\partial A'_{ik}}= \\begin{cases}\n",
    "\\alpha_{ij}(1-\\alpha_{ij}),& j = k\\\\\n",
    "-\\alpha_{ij}\\alpha_{ik},& j\\neq k\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A'_{ij}}=\\sum_b\\dfrac{\\partial L}{\\partial \\alpha_{ib}}\\dfrac{\\partial \\alpha_{ib}}{\\partial A'_{ij}}\n",
    "$$\n",
    "\n",
    "note that we can factorize $\\alpha_{ij}$ in the above expression, therefore it is zero.\n",
    "\n",
    "For $A$,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A_{ij}}=\\sum_{ab}\\dfrac{\\partial L}{\\partial A'_{ab}}\\dfrac{\\partial A'_{ab}}{\\partial A_{ij}}= \\dfrac{\\partial L}{\\partial A'_{ij}}=\\dfrac{\\partial L}{\\partial M_{ij}}\n",
    "$$\n",
    "\n",
    "note that $M$ is not learnable so we turn off the gradients for $M$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
