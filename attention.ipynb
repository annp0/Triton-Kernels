{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa7bd6f",
   "metadata": {},
   "source": [
    "### -1. Back prop for matmul\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "Y = WX, \\quad y_{ij} = \\sum_a w_{ia}x_{aj}\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial y_{ij}}{\\partial x_{mn}}&=\\dfrac{\\partial}{\\partial x_{mn}}\\left(\\sum_aw_{ia}x_{aj}\\right)\\\\\n",
    "&=\\sum_a\\dfrac{\\partial}{\\partial x_{mn}}w_{ia}x_{aj}\\\\\n",
    "&=\\sum_a w_{ia}\\delta_{m,a}\\delta_{n,j}\\\\\n",
    "&=w_{im}\\delta_{n,j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial x_{mn}}&=\\sum_{ij}\\dfrac{\\partial L}{\\partial y_{ij}}\\dfrac{\\partial y_{ij}}{\\partial x_{mn}}\\\\\n",
    "&=\\sum_{ij}\\dfrac{\\partial L}{\\partial y_{ij}}w_{im}\\delta_{n,j}\\\\\n",
    "&=\\sum_{i}\\dfrac{\\partial L}{\\partial y_{in}}w_{im}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\left(\\dfrac{\\partial L}{\\partial X}\\right)=W^T\\left(\\dfrac{\\partial L}{\\partial Y}\\right)\n",
    "$$\n",
    "\n",
    "and similarly,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial W}=\\dfrac{\\partial L}{\\partial Y} X^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd752e1",
   "metadata": {},
   "source": [
    "#### 0. Input and embedding\n",
    "\n",
    "Let:\n",
    "\n",
    "* $I \\in \\mathbb{Z}^{T}$: token indices of the input sequence\n",
    "* Embedding matrix: $E_{\\text{lookup}} \\in \\mathbb{R}^{d_{\\text{token}} \\times d_{\\text{model}}}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "X = E_{\\text{lookup}}[I] \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "This means that each token index in $I$ is used to select a row from $E_{\\text{lookup}}$.\n",
    "\n",
    "Note that this is not a matmul (it can be written as a matmul if $I$ was one-hot encoded), it is just a lookup. However, autograd engines can track which row it used and back prop gradients to that row.\n",
    "\n",
    "#### 1. Linear Projections\n",
    "\n",
    "We define three learnable projection matrices:\n",
    "\n",
    "* $W_Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "* $W_K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "* $W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n",
    "\n",
    "Then the projections are:\n",
    "\n",
    "$$\n",
    "Q = X W_Q \\in \\mathbb{R}^{T \\times d_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K = X W_K \\in \\mathbb{R}^{T \\times d_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V = X W_V \\in \\mathbb{R}^{T \\times d_v}\n",
    "$$\n",
    "\n",
    "The dimension $k$ is usually taken to be even for RoPE and acceleration. Each token corresponds to a row.\n",
    "\n",
    "#### 2. RoPE\n",
    "\n",
    "Positional Embeddings are necessary to make sure the model is position-ware to inputs. For each token position $t\\in\\{1,\\cdots,T\\}$, and each dimension pair $i\\in\\{1,\\cdots, d_k/2\\}$ (arrays start from 1), define\n",
    "\n",
    "$$\n",
    "q_{t,i} = \\left[\\begin{matrix}\n",
    "Q_{t, 2i-1}\\\\ Q_{t,2i}\n",
    "\\end{matrix}\\right],\\quad\n",
    "k_{t,i} = \\left[\\begin{matrix}\n",
    "K_{t, 2i-1}\\\\ K_{t,2i}\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "and define\n",
    "\n",
    "$$\n",
    "R(i, t) = \\left[\\begin{matrix}\n",
    "\\cos(\\omega_i t) & -\\sin(\\omega_i t)\\\\\n",
    "\\sin(\\omega_i t) & \\cos(\\omega_i t)\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "q'_{t,i}=R(i, t)q_{t,i},\\quad k'_{t,i}=R(i, t)k_{t,i}\n",
    "$$\n",
    "\n",
    "and we put it back into $Q', K'$.\n",
    "\n",
    "We use RoPE mainly because of the property where the dot product only relies on the original dot product and the relative position.\n",
    "\n",
    "#### 3. Naive Attention\n",
    "\n",
    "We compute attention weights:\n",
    "\n",
    "$$\n",
    "A = \\frac{Q' K'^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{T \\times T}\n",
    "$$\n",
    "\n",
    "and $A_{ij}$ is basically the dot product of the query of token $i$ and the key of token $j$. So now the $i$-th row of $A$ is the 'score' of keys of other tokens against token $i$. We apply row-wise softmax to convert them to weights that adds to 1:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp\\left( A_{ij} \\right)}{\\sum_{k=1}^{T} \\exp\\left( A_{ik} \\right)}\n",
    "$$\n",
    "\n",
    "then we use this to average against $V$:\n",
    "\n",
    "$$\n",
    "O = \\alpha V \\in \\mathbb{R}^{T\\times d_v}\n",
    "$$\n",
    "\n",
    "#### 4. Masks\n",
    "\n",
    "Before applying softmax, we may need to mask out certain entries in $A$ to force better behaviour.\n",
    "\n",
    "(a) Causal Mask: For auto-regressive generation, the output of token $i$ should only use the values of tokens $j\\le i$, since the future is not avaliable at inference-time. Define \n",
    "\n",
    "$$\n",
    "(M_c)_{ij}=\\begin{cases}\n",
    "0, & j\\le i\\\\\n",
    "-\\inf, &j > i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and let $A'=A+M_c$, this effective turns the weights / scores of masked entries to zero.\n",
    "\n",
    "(b) Padding Mask: If padding tokens were to be used, they would be meaningless, therefore, we zero out every entry whose indices contains a padding token.\n",
    "\n",
    "For the autograd engine, the gradient will only flow to non-masked entries:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial O} V^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A'_{ij}}=\\sum_{ab}\\dfrac{\\partial L}{\\partial \\alpha_{ab}}\\dfrac{\\partial \\alpha_{ab}}{\\partial A'_{ij}}\n",
    "$$\n",
    "\n",
    "Since the softmax was applied row-wise, within the same row we have\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\alpha_{ij}}{\\partial A'_{ik}}= \\begin{cases}\n",
    "\\alpha_{ij}(1-\\alpha_{ij}),& j = k\\\\\n",
    "-\\alpha_{ij}\\alpha_{ik},& j\\neq k\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A'_{ij}}=\\sum_b\\dfrac{\\partial L}{\\partial \\alpha_{ib}}\\dfrac{\\partial \\alpha_{ib}}{\\partial A'_{ij}}\n",
    "$$\n",
    "\n",
    "note that we can factorize $\\alpha_{ij}$ in the above expression, therefore it is zero.\n",
    "\n",
    "For $A$,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial A_{ij}}=\\sum_{ab}\\dfrac{\\partial L}{\\partial A'_{ab}}\\dfrac{\\partial A'_{ab}}{\\partial A_{ij}}= \\dfrac{\\partial L}{\\partial A'_{ij}}=\\dfrac{\\partial L}{\\partial M_{ij}}\n",
    "$$\n",
    "\n",
    "note that $M$ is not learnable so we turn off the gradients for $M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2439531",
   "metadata": {},
   "source": [
    "#### 5. Multi-head Attention, Multi-query Attention, and Grouped-query Attention\n",
    "\n",
    "Split $d_k$, $d_v$ into $h$ heads:\n",
    "\n",
    "$$\n",
    "d_k=h\\times d^h_k,\\quad d_v=h\\times d^h_v\n",
    "$$\n",
    "\n",
    "define per-head projection matrices for queries, keys, values for $m\\in\\{1,\\cdots,h\\}$:\n",
    "\n",
    "$$\n",
    "W_Q^m\\in\\mathbb{R}^{d_{model}\\times d_k^h},\\quad W_K^m\\in\\mathbb{R}^{d_{model}\\times d_k^h},\\quad W_V^m\\in\\mathbb{R}^{d_{model}\\times d_v^h}\n",
    "$$\n",
    "\n",
    "Then for each input $X\\in\\mathbb{R}^{T\\times d_{model}}$, we have\n",
    "\n",
    "$$\n",
    "Q^m=XW_Q^m,\\quad K^m=XW^m_K,\\quad V^m=XW^m_V\n",
    "$$\n",
    "\n",
    "and we apply RoPE for each head ($Q^m$, $K^m$), then apply attention and softmax for each head, getting $O^m\\in\\mathbb{R}^{T\\times d^h_v}$. Concatenating all heads gives $O\\in \\mathbb{R}^{T\\times d_v}$.\n",
    "\n",
    "For multi-query attention, each head only have their own $Q^m\\in \\mathbb{R}^{T\\times d^h_k}$, and share $K\\in  \\mathbb{R}^{T\\times d^h_k}$ and $V\\in \\mathbb{R}^{T\\times d^h_v}$. \n",
    "\n",
    "For grouped-query attention, we divide the set of heads $\\{1,\\cdots,h\\}$ into $g$ disjoint groups, with each group sharing a set of keys and values. Queries remain per-head as in MHA and MQA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fccc520",
   "metadata": {},
   "source": [
    "#### 6. Back Prop (The intended way)\n",
    "\n",
    "Given $O=\\sigma(A-M)V=DV$, where $A = QK^T/\\sqrt{d_k}$, we deduce:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial V} = D^T\\dfrac{\\partial L}{\\partial O}\n",
    "$$\n",
    "\n",
    "As for $Q, K$, first let $A=QK^T/\\sqrt{d_k}$, then by the back prop of matmul, we have\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial Q}=\\dfrac{1}{\\sqrt{d_k}}\\left(\\dfrac{\\partial L}{\\partial A}\\right)K\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial K}=\\left(\\dfrac{\\partial L}{\\partial K^T}\\right)^T=\\dfrac{1}{\\sqrt{d_k}}\\left(\\dfrac{\\partial L}{\\partial A}\\right)^TQ\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial D} = \\dfrac{\\partial L}{\\partial O} V^T\n",
    "$$\n",
    "\n",
    "for softmax, we also have\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial D_{ab}}{\\partial A_{cd}}=\\delta_{a,c}D_{ab}(\\delta_{b,d}-D_{ad})\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial A_{cd}}&=\\sum_{ab}\\dfrac{\\partial L}{\\partial D_{ab}}\\dfrac{\\partial D_{ab}}{\\partial A_{cd}}\\\\\n",
    "&=\\sum_b\\dfrac{\\partial L}{\\partial D_{cb}}D_{cb}(\\delta_{b,d}-D_{cd})\\\\\n",
    "&=D_{cd}\\left(\\dfrac{\\partial L}{\\partial D_{cd}}-\\sum_bD_{cb}\\dfrac{\\partial L}{\\partial D_{cb}}\\right)\\\\\n",
    "&=D_{cd}\\left(\\sum_{m}\\dfrac{\\partial L}{\\partial O_{cm}}V_{dm}-\\sum_bD_{cb}\\sum_n\\dfrac{\\partial L}{\\partial O_{cm}}V_{bm}\\right)\\\\\n",
    "&=D_{cd}\\left(\\sum_{m}\\dfrac{\\partial L}{\\partial O_{cm}}V_{dm}-\\sum_n\\dfrac{\\partial L}{\\partial O_{cn}}O_{cn}\\right)\\\\\n",
    "&=D_{cd}\\left(\\left(\\dfrac{\\partial L}{\\partial D}\\right)_{cd}-\\sum_n\\dfrac{\\partial L}{\\partial O_{cn}}O_{cn}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This allows us to calculate all the gradients by propagating through $A$.\n",
    "\n",
    "##### 6.0. Masking\n",
    "\n",
    "Causal masking ensures that the $m$-output token $O_m$ only depends on $K$ and $V$ positions $n\\le m$. That is, each query attends only to the past or current tokens. Therefore the gradient $\\frac{\\partial L}{\\partial V_n}$ and $\\frac{\\partial L}{\\partial K_n}$ only receive contributions from $O_m$ with $m\\ge n$.\n",
    "\n",
    "The gradient $\\frac{\\partial L}{\\partial Q_m}$ only depends on $O_m$, since attention depends on the current query only.\n",
    "\n",
    "\n",
    "##### 6.1. The un-intended way\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial Q_{ij}}&=\\sum_{abcd}\\dfrac{\\partial L}{\\partial D_{ab}}\\dfrac{\\partial D_{ab}}{\\partial F_{cd}}\\dfrac{\\partial F_{cd}}{\\partial Q_{ij}}\\\\\n",
    "&=\\sum_{abcde}\\dfrac{\\partial L}{\\partial O_{ae}}V_{be}\\delta_{a,c}D_{ab}(\\delta_{b,d}-D_{ad})\\dfrac{1}{\\sqrt{d_k}}\\delta_{c,i}K_{dj}\\\\\n",
    "&=\\sum_{bde}\\dfrac{\\partial L}{\\partial O_{ie}}V_{be}D_{ib}(\\delta_{b,d}-D_{id})\\dfrac{1}{\\sqrt{d_k}}K_{dj}\\\\\n",
    "&=\\dfrac{1}{\\sqrt{d_k}}\\sum_{be}\\dfrac{\\partial L}{\\partial O_{ie}}V_{be}D_{ib}\\left(K_{bj}-\\sum_dD_{id}K_{dj}\\right)\\\\\n",
    "&=\\dfrac{1}{\\sqrt{d_k}}\\left(\\sum_{be}\\dfrac{\\partial L}{\\partial O_{ie}}V_{be}D_{ib}K_{bj}\\right)-\\left(\\sum_d D_{id}K_{dj}\\right)\\Delta_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial K_{ij}} \n",
    "&= \\sum_{abcd} \\dfrac{\\partial L}{\\partial D_{ab}} \\dfrac{\\partial D_{ab}}{\\partial F_{cd}} \\dfrac{\\partial F_{cd}}{\\partial K_{ij}} \\\\\n",
    "&= \\sum_{abcde} \\dfrac{\\partial L}{\\partial O_{ae}} V_{be} \\delta_{a,c} D_{ab} (\\delta_{b,d} - D_{ad}) \\dfrac{1}{\\sqrt{d_k}} \\delta_{d,i} Q_{cj} \\\\\n",
    "&= \\sum_{abe} \\dfrac{\\partial L}{\\partial O_{ae}} V_{be} D_{ab} (\\delta_{b,i} - D_{ai}) \\dfrac{1}{\\sqrt{d_k}} Q_{aj} \\\\\n",
    "&= \\dfrac{1}{\\sqrt{d_k}} \\sum_{ae} \\dfrac{\\partial L}{\\partial O_{ae}} D_{ai} Q_{aj} \\left( V_{ie} - \\sum_b D_{ab} V_{be} \\right)\\\\\n",
    "&= \\dfrac{1}{\\sqrt{d_k}} \\sum_{a} D_{ai} Q_{aj}\\sum_{e} \\dfrac{\\partial L}{\\partial O_{ae}} \\left( V_{ie} - O_{ae} \\right)\\\\\n",
    "&= \\dfrac{1}{\\sqrt{d_k}} \\sum_{a} D_{ai} Q_{aj}\\left(\\left( \\sum_{e} \\dfrac{\\partial L}{\\partial O_{ae}}V_{ie}\\right) - \\Delta_a\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Delta_k=\\sum_l\\dfrac{\\partial L}{\\partial O_{kl}}O_{kl}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1104df",
   "metadata": {},
   "source": [
    "#### 7. GPU-Optimization: FlashAttention\n",
    "\n",
    "On GPU, computing $\\sigma(QK^T/\\sqrt{d_k})V$ is slow because those are big matrices (cannot fit in SRAM) and the amount of computation and memory is quadratic over $d^h_k$.\n",
    "\n",
    "The computation we need to do is for a given query $Q[i]$, compute dot products with all keys $K[j]$, apply softmax, then take a weighted sum over $V[j]$.\n",
    "\n",
    "Our approach is to:\n",
    "\n",
    "(1) Increase memory throughput: let each block compute BLOCK_M rows of queries, and within each block, we iterate (stream) over all of $K$ and corresponding $V$ (with row-tiling size BLOCK_N). Since every block is loading the same K, V-tiles, they might stay hot in the L2 cache. Also, most of the masked regions are never calculated.\n",
    "\n",
    "(2) Fuse Operations. Results are not written back to DRAM until the computation is complete."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
